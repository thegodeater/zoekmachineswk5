{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  Practice Text classification with Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Text classification with Naive Bayes  \n",
    "        \n",
    "        \n",
    "        \n",
    "<h3>Abstract</h3>\n",
    "<p>We will do text classification on a collection of Dutch parliamentary questions.\n",
    "    The website <a href=\"https://zoek.officielebekendmakingen.nl/zoeken/parlementaire_documenten\">officielebekendmakingen.nl</a>lets you search in \"kamervragen\".\n",
    "    You can donwload\n",
    "    <a href='http://data.politicalmashup.nl/kamervragen/PoliDocs_Kamervragen.zip'>this zipfile with Kamervragen in XML</a>\n",
    "    to see some of the  data in XML format. \n",
    "    It also contains style sheets to show the XML well in a browser.  \n",
    "    The <a href='http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/'>MYSQL directory</a> contains an <a href='http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR14807.xml'>example   Kamervraag XML file</a> and a file `kvr.csv.gz` with all those kamervragen in a handy csv format. Note that in your browser you see the result of applying stylesheets. So choose View Source or open it in an editor.</p>\n",
    "\n",
    "<h3>First exploration</h3>\n",
    "\n",
    "See below.\n",
    "\n",
    "<h2>Exercise</h2>\n",
    "\n",
    "<p>We will use the fields in elements of the form <tt> &lt;item attribuut=\"Afkomstig_van\"></tt> as our classes. \n",
    "    These are the ministeries to whom the question is addressed.\n",
    "    An example is \n",
    "    <pre>\n",
    "        &lt;item attribuut=\"Afkomstig_van\">Landbouw, Natuurbeheer en Visserij (LNV)&lt;/item>\n",
    "    </pre>\n",
    "    Note that these labels are <strong>not normalized</strong>, see e.g. the counts below:\n",
    "    <pre>\n",
    "Justitie (JUS)                                                   3219\n",
    "Volksgezondheid, Welzijn en Sport (VWS)                          2630\n",
    "Buitenlandse Zaken (BUZA)                                        1796\n",
    "Verkeer en Waterstaat (VW)                                       1441\n",
    "Justitie                                                         1333\n",
    "Sociale Zaken en Werkgelegenheid (SZW)                           1231\n",
    "Onderwijs, Cultuur en Wetenschappen (OCW)                        1187\n",
    "Volkshuisvesting, Ruimtelijke Ordening en Milieubeheer (VROM)     984\n",
    "FinanciÃ«n (FIN)                                                   960\n",
    "Volksgezondheid, Welzijn en Sport                                 951\n",
    "Economische Zaken (EZ)                                            946\n",
    "Buitenlandse Zaken                                                753\n",
    "Binnenlandse Zaken en Koninkrijksrelaties (BZK)                   725\n",
    "Verkeer en Waterstaat                                             724\n",
    "Defensie (DEF)                                                    646\n",
    "Sociale Zaken en Werkgelegenheid                                  607\n",
    "Landbouw, Natuurbeheer en Visserij (LNV)                          586\n",
    "Volkshuisvesting, Ruimtelijke Ordening en Milieubeheer            554\n",
    "Onderwijs, Cultuur en Wetenschappen                               532\n",
    "Vreemdelingenzaken en Integratie (VI)                             466\n",
    "    </pre>\n",
    "</p>\n",
    "\n",
    "  <ol>\n",
    "      <li>Normalize the values for \"ministerie\" and choose 10 ministeries to work with. </li>\n",
    "      <li>Implement the two algorithms in Fig MRS.13.2, using your earlier code for creating term and document frequencies.\n",
    "      It might be easier to use the representation and formula given in MRS section 13.4.1.</li>\n",
    "      <li>On this collection, train NB text classifiers for 10 different classes with enough and interesting data.</li>\n",
    "      <li>Compute for each term and each of your 10 classes its utility for that class using mutual information.</li>\n",
    "      <li>For each class, show the top 10 words as in Figure 13.7 in MRS.</li>\n",
    "      <li>Evaluate your classifiers using Precision, Recall and F1. (\n",
    "           <br/>\n",
    "          Give a table in which you show these values for using the top 10, top 100 terms and all terms, for all of your 10 classes.\n",
    "          Thus do feature selection per class, and use for each class the top n best features for that class. \n",
    "          <br/>\n",
    "      Also show the microaverage(s) for all 10 classes together.\n",
    "      <br/>\n",
    "      If you like you can also present this in a figure like MRS.13.8. \n",
    "      Then compute the F1 measure for the same number of terms as in that figure.</li>\n",
    "      <li>Reflect and report briefly about your choices in this process and about the obtained results. </li>\n",
    "  </ol>\n",
    "\n",
    "<h3>Training/Testing</h3>\n",
    "<p>It is important that you do not test your classifier using documents that have also been used in training.\n",
    "    So split up your collection in a training set and a test set. A 80%-20% split is reasonable.\n",
    "\n",
    "<br/>\n",
    "    If you have too little data you can use 5 or <a href=\"http://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\">10-fold cross validation</a>.</p>\n",
    "\n",
    "<h2>Form of presentation</h2>\n",
    "<ul>\n",
    "    <li>Make slides or wikipages and have your system running (this could just be an IPython notebook with a classify function that accepts any string and classifies it.) ~~and be able to accept documents from the web.~~ </li>\n",
    "    <li>Create one or two slides or wikipages for each of the sub exercises listed above.\n",
    "</li>\n",
    "<li>Make it clear in the heading of the slides which sub exercises you talk about.</li>\n",
    "    <li>Show running code with one or two  good examples (a TP of course, but also a FP and an error-analysis is nice to show). </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "<h2>Form of handing in your final product</h2>\n",
    "<ul>\n",
    "    <li>An IPython notebook would be perfect, with clear indications which part of the code answers which subquestion.</li>\n",
    "    <li>A clear git repo, with good comments and a clear separation and indication what code does what is also fine.</li>\n",
    "    <li> You are free to program in whatever language you prefer.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import timeit\n",
    "import nltk\n",
    "import math\n",
    "import numpy\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter, defaultdict\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"You've already run this code!\")? (<ipython-input-1-4305cbff1829>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-4305cbff1829>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    print \"You've already run this code!\"\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"You've already run this code!\")?\n"
     ]
    }
   ],
   "source": [
    "# Change to KVR1000.csv.gz if this becomes too slow for you\n",
    "kvrdf= pd.read_csv('http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR.csv.gz', \n",
    "                   compression='gzip', sep='\\t', encoding='utf-8',\n",
    "                   index_col=0, names=['jaar', 'partij','titel','vraag','antwoord','ministerie']) \n",
    "\n",
    "\n",
    "# Normalize the values for \"ministerie\" and choose 10 ministeries to work with.\n",
    "ourMinisteries={\"ustitie\":\"JUS\", \"nderwijs\":\"OCW\", \"uitenlandse\":\"BUZA\", \"innenlandse\":\"BZK\", \"efensie\":\"DEF\", \"reemdeling\":\"VI\", \"andbouw\":\"LNV\", \"erkeer\":\"VW\", \"olkshuisvesting\":\"VROM\", \"olksgezondheid\":\"VWS\"}\n",
    "ourNormMinisteries=[\"JUS\", \"OCW\", \"BUZA\", \"BZK\", \"DEF\", \"VI\", \"LNV\", \"VW\", \"VROM\", \"VWS\"]\n",
    "check = True\n",
    "for index, row in kvrdf.iterrows():\n",
    "    if row.ministerie in ourNormMinisteries:\n",
    "        if check:\n",
    "            print \"You've already run this code!\" \n",
    "            check = False\n",
    "    elif isinstance(row.ministerie, basestring) and any(x in row.ministerie for x in ourMinisteries):\n",
    "        for key in ourMinisteries:\n",
    "            if key in row.ministerie:\n",
    "                kvrdf.set_value(index, 'ministerie', ourMinisteries[key])\n",
    "                break\n",
    "    else:\n",
    "        kvrdf=kvrdf.drop([index])\n",
    "\n",
    "print kvrdf.shape\n",
    "# Divide data into test and training sets:\n",
    "kvrTrain = kvrdf[4201:]\n",
    "kvrTest = kvrdf[:4200]\n",
    "print kvrTrain.shape, kvrTest.shape\n",
    "\n",
    "# List of stopwords and special chars\n",
    "DutchStop = stopwords.words('dutch') + [u',',u'.',u'?',u')',u'(',u\"''\",u'-']\n",
    "\n",
    "# List the ministeries from most to least common.\n",
    "counts = kvrTrain.ministerie.value_counts()\n",
    "\n",
    "# Show the classes we will be using\n",
    "classList = list(counts[:10].axes[0][0:10])\n",
    "print classList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Indexing...\n",
      "1 Done!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n"
     ]
    }
   ],
   "source": [
    "# From the previous assignment: Making an inverted index\n",
    "def index_collection(docs):\n",
    "    MyIndex= defaultdict(Counter) # initialize MyIndex\n",
    "    print 'Indexing...'\n",
    "    count = len(docs)\n",
    "    for index, doc in docs.iterrows():  # loop over each file\n",
    "        print \"\\r\", count,\n",
    "        count -= 1\n",
    "        f = doc.titel + doc.vraag + doc.antwoord\n",
    "        text = [w for w in nltk.word_tokenize(f)]   # get text and tokenize\n",
    "        for w in text:    # update MyIndex with each token \n",
    "            if not w.isupper(): # leave completely upper case words\n",
    "                w = w.lower()   # lower case others\n",
    "            MyIndex[w][index]+=1\n",
    "    print 'Done!'            \n",
    "    return MyIndex\n",
    "\n",
    "trainingIndex = index_collection(kvrTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197177"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the two algorithms\n",
    "\n",
    "# Training function\n",
    "def trainMNB(classes, docs):\n",
    "    print 'Training...'\n",
    "    # Extract vocabulary v\n",
    "    ## DEPRECATED ##\n",
    "    # v = vocab(docs)\n",
    "    vsize = len(trainingIndex)\n",
    "    print 'vocab size: ', vsize\n",
    "    # n is the number of documents used\n",
    "    n = len(docs)\n",
    "    # for each class do:\n",
    "    p_nc = {}\n",
    "    condProb = {}\n",
    "    print 'Training class: '\n",
    "    for c in classes:\n",
    "        print c\n",
    "        condProb[c] = {}\n",
    "        # Count documents in class (docs, c)\n",
    "        nc = docs.ministerie.value_counts().loc[c]\n",
    "        # Calculate the prior\n",
    "        p_nc[c] = nc / n\n",
    "        # Get all text for the class\n",
    "        t_counts = Counter(classText(docs, c))\n",
    "        t_total = sum(t_counts.values())\n",
    "        # Calculate conditional probability\n",
    "        time = len(trainingIndex)\n",
    "        for (t, counter) in trainingIndex.items():\n",
    "            print '\\r', time,\n",
    "            time -= 1\n",
    "            if t in t_counts:\n",
    "                condProb[c][t] = (t_counts[t] + 1) / (t_total + vsize)\n",
    "            else:\n",
    "                condProb[c][t] = 1 / (t_total + vsize)\n",
    "    print 'Done!'\n",
    "    return p_nc, condProb\n",
    "\n",
    "# Get all words from a set of documents as a list of strings\n",
    "def vocab(docs):\n",
    "    result = []\n",
    "    # Get text from these index keys:\n",
    "    text = docs.titel + docs.vraag + docs.antwoord\n",
    "    # Tokenize each string and turn into a bag of words\n",
    "    count = 0\n",
    "    print 'Processing documents:'\n",
    "    for item in text:\n",
    "        count += 1\n",
    "        print '\\r', len(text)-count,\n",
    "        BoW = list([w for w in nltk.word_tokenize(item) if not w in set(DutchStop)])\n",
    "        result += BoW\n",
    "    return result\n",
    "\n",
    "# Get all words for a class and set of documents as a list of strings\n",
    "def classText(docs, c):\n",
    "    result = []\n",
    "    classdocs = docs[docs.ministerie==c]\n",
    "    text = classdocs.titel + classdocs.vraag + classdocs.antwoord\n",
    "    # Tokenize and turn into list\n",
    "    for item in text:\n",
    "        BoW = list([w for w in nltk.word_tokenize(item) if not w in set(DutchStop)])\n",
    "        result += BoW\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "vocab size:  197177\n",
      "Training class: \n",
      "VIFM         Done!\n"
     ]
    }
   ],
   "source": [
    "# Training the classifier\n",
    "# list(set(vocab(kvrTest[0:100])))\n",
    "p, cp = trainMNB(classList, kvrTrain)\n",
    "#timeit.timeit(trainMNB(classList, kvrTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the trained model to a document\n",
    "def applyMNB(classes, prior, cp, doc):\n",
    "    score = {}\n",
    "    w = nltk.word_tokenize(doc.titel + doc.vraag + doc.antwoord)\n",
    "    for c in classes:\n",
    "        score[c] = math.log(prior[c])\n",
    "        for t in w:\n",
    "            if t in cp[c]:\n",
    "                # print '\\r', score[c],\n",
    "                score[c] += math.log(cp[c][t])\n",
    "    # print score\n",
    "    return max(score, key=lambda i: score[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the MNB \n",
    "# --unfinished--\n",
    "def applySet(docs,p,cp):\n",
    "    correct = 0\n",
    "    mnbCount = defaultdict(Counter)\n",
    "    time = len(docs)\n",
    "    print 'Documents remaining:'\n",
    "    for index, doc in docs.iterrows():\n",
    "        print '\\r', time,\n",
    "        time -= 1\n",
    "        mnb = applyMNB(classList, p, cp, doc)\n",
    "        if mnb == doc.ministerie:\n",
    "            mnbCount[mnb]['correct'] += 1\n",
    "            correct += 1\n",
    "        else:\n",
    "            mnbCount[mnb]['incorrect'] += 1\n",
    "    print 'precision: ', correct / len(docs)\n",
    "    return mnbCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents remaining:\n",
      "1 precision:  0.262380952381                                                                                                                                                                                                    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'BUZA': Counter({'correct': 36}),\n",
       "             'BZK': Counter({'correct': 165, 'incorrect': 106}),\n",
       "             'DEF': Counter({'correct': 172, 'incorrect': 639}),\n",
       "             'JUS': Counter({'correct': 1}),\n",
       "             'LNV': Counter({'correct': 209, 'incorrect': 93}),\n",
       "             'OCW': Counter({'correct': 191, 'incorrect': 13}),\n",
       "             'VI': Counter({'incorrect': 2163}),\n",
       "             'VROM': Counter({'correct': 218, 'incorrect': 73}),\n",
       "             'VW': Counter({'correct': 102, 'incorrect': 11}),\n",
       "             'VWS': Counter({'correct': 8})})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try classifying a document:\n",
    "#row = kvrTest.ix[4]\n",
    "#applyMNB(classList, p, cp, row)\n",
    "\n",
    "# Classify a set of documents\n",
    "applySet(kvrTest,p,cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute for each term and each of your 10 classes its utility for that class using mutual information. \n",
    "# --unfinished--\n",
    "def mutualInfo(clist, docs, index):\n",
    "    result = {}\n",
    "    for c in clist:\n",
    "        v = list(set(classText(docs, c)))\n",
    "        result[c] = {}\n",
    "        print 'Processing terms for: ', c\n",
    "        time = len(index)\n",
    "        for (t, counter) in index:\n",
    "            time -= 1\n",
    "            print '\\r', time,\n",
    "            classDocs = docs.ministerie.loc[c]\n",
    "            n = len(classDocs)\n",
    "            n1 = len(counter)\n",
    "            n0 = len(docs) - n1\n",
    "            n11, n10, n01, n00 = (1,)*4\n",
    "            for doc, count in counter.items():\n",
    "                if t in tokens:\n",
    "                    n1 += 1\n",
    "                    if doc.ministerie == c:\n",
    "                        n11 += 1\n",
    "                    else:\n",
    "                        n10 += 1\n",
    "                else:\n",
    "                    n0 += 1\n",
    "                    if doc.ministerie == c:\n",
    "                        n01 += 1\n",
    "                    else:\n",
    "                        n00 += 1\n",
    "            # print n, n1, n0, n11, n10, n01, n00\n",
    "            t1 = (n11 / n) * math.log((n*n11)/(n1*(n01+n11)),2)\n",
    "            t2 = (n01/n) * math.log((n*n01)/((n00+n01)*(n01+n11)),2)\n",
    "            t3 = (n10/n) * math.log((n*n10)/((n10+n11)*(n00+n10)),2)\n",
    "            t4 = (n00/n) * math.log((n*n00)/((n00+n01)*(n00+n10)),2)\n",
    "            # print t1, t2, t3, t4\n",
    "            result[c][t] = t1 + t2 + t3 + t4\n",
    "            # print '\\r', result\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing terms \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "#print len(list(set(v)))\n",
    "mi = mutualInfo([classList[0]], kvrTrain[:10])\n",
    "print sorted(mi['JUS'].items(), key=itemgetter(1), reverse = True)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JUS     3687\n",
       "VWS     3009\n",
       "BUZA    2184\n",
       "VW      1743\n",
       "OCW     1606\n",
       "BZK     1457\n",
       "VROM    1217\n",
       "LNV     1186\n",
       "DEF      873\n",
       "VI       554\n",
       "Name: ministerie, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvrTrain.ministerie.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
